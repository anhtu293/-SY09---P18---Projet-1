library(lattice)
library(corrplot)
library("FactoMineR")
library(factoextra)
library(ggplot2)
library(tibble)
library(ade4)
library(MASS)
library(ggrepel)
library(cluster)
setwd("D:/Bi/utc/SY09/Projet 1")
recette_pays <- read.table("recettes-pays.data", header = TRUE, sep = ",", row.names = "origin")
setwd("D:/Bi/utc/sy09/Projet1 _ Anh Tu NGUYEN - Marie Valmori")
recette_pays <- read.table("recettes-pays.data", header = TRUE, sep = ",", row.names = "origin")
recette_pays <- read.table("recettes-pays.data", header = TRUE, sep = ",", row.names = "origin")
#Question 1 : Analyse exploratoire
#analyses simples
str(recette_pays)
summary(recette_pays)
variance<-data.frame(variance=sapply(recette_pays,var))
variance<-t(variance)
covariance <- cov(recette_pays)
summary(covariance)
#test de corrélation
correlation <- cor(recette_pays)
corrplot(correlation, method="circle", type="lower")
dev.off()
cor.mtest <- function(mat) {
mat <- as.matrix(mat)
n <- ncol(mat)
p.mat<- matrix(NA, n, n)
diag(p.mat) <- 0
for (i in 1:(n )) {
for (j in (i):n) {
tmp <- cor.test(mat[, i], mat[, j])
p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
}
}
colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
p.mat
}
p.mat<-cor.mtest(correlation)
x11()
corrplot(correlation, type="lower", order="hclust", p.mat = p.mat, sig.level = 0.05,  insig = "blank")
summary(correlation)
#observation des boxplots selon toutes les variables
x11()
boxplot(recette_pays[,1:16], ylim=c(0,1), main="Boxplot 1" )
boxplot(recette_pays[,17:27], ylim=c(0,1),main="Boxplot 2")
boxplot(recette_pays[,28:39],ylim=c(0,1), main="Boxplot 3")
boxplot(recette_pays[,40:50],ylim=c(0,1), main="Boxplot 4")
#Question 2:
##Analyse composants principales
rec_matrice <- as.matrix(recette_pays[,-1])
Dp <- diag(x = 1/26, nrow = 26, ncol = 26) # 26 observations
M <- diag(x = 1, nrow = 50, ncol = 50)
rec_scaled <- scale(rec_matrice)
V <- t(rec_scaled)%*%Dp%*%rec_scaled
x <- eigen(V)
valeur_propre <- x$values
vecteur_propre <- x$vectors
positive <- valeur_propre > 0 #valeurs propres positives
valeur_propre <- valeur_propre[positive == TRUE]
vecteur_propre <- vecteur_propre[,1:length(valeur_propre)]
barplot(valeur_propre)
inertie_totale <- sum(valeur_propre)
pourcentage_iner_expliquee <- valeur_propre / inertie_totale
C <- rec_scaled%*%M%*%vecteur_propre
plot(C[,2]~C[,1])
text(C[,1],C[,2],recette_pays[,1])
plot(C[,3]~C[,1])
text(C[,1],C[,3],recette_pays[,1])
D <- cor(rec_scaled, C)
plot(-1:1, -1:1, type = "n", xlab = "Axe factoriel 1", ylab = "Axe factoriel 2")
text(D[, 1], D[, 2],colnames(rec_matrice))
abline(h = 0, v = 0, col = "blue")
curve(sqrt(25 - x^2), -1, 1, add = T, col = "blue")
curve(-sqrt(5 - x^2), -1, 1, add = T, col = "blue")
#essai bis ACP
x11()
par(mfrow=c(2,2))
res.pca <- PCA(recette_pays, graph = FALSE)
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50),  main = "Valeurs propres")
fviz_ca_biplot(res.pca, addlabels = TRUE, ylim = c(0, 50))
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
fviz_pca_var(res.pca, col.var = "black")
fviz_pca_var(res.pca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE )
fviz_pca_ind (res.pca, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE )# Évite le chevauchement de texte
fviz_pca_ind(res.pca,
geom.ind = "point",  scale(1,1),# colorer by groups
palette = c("#00AFBB", "#E7B800", "#FC4E07"),
addEllipses = TRUE, # Ellipses de concentration
legend.title = "Groups"
)
fviz_pca_biplot(res.pca, repel = TRUE,
col.var = "#2E9FDF", # Couleur des variables
col.ind = "#696969"  # Couleur des individues
)
rec_matrice <- as.matrix(recette_pays)
rec_scaled <- scale(rec_matrice)
dissimilarite <- dist(rec_scaled, method = "manhattan", diag = TRUE)
par(mfrow = c(1,2))
hc_cusine.ward <- hclust(dissimilarite, method ="ward.D")
x11()
plot(hc_cusine.ward, main = "Method : Ward D")
inertie <- sort(hc_cusine.ward$height, decreasing = TRUE)
plot(inertie[1:20], type = "s", xlab = "Nombre de classes", ylab = "Inertie")
points(c(2, 3, 8), inertie[c(2, 3, 8)], col = c("yellow", "purple", "greeen"), cex = 2,
lwd = 3)
###méthode wardd2
x11()
hc_cusine.wardd2 <- hclust(dissimilarite, method = "ward.D2")
plot(hc_cusine.wardd2, main = "Method : Ward D2") #carré dees distances
inertie <- sort(hc_cusine.wardd2$height, decreasing = TRUE)
plot(inertie[1:20], type = "s", xlab = "Nombre de classes", ylab = "Inertie", main = "Sauts d'inertie mathode de Ward")
points(c(2, 3, 8), inertie[c(2, 3, 8)], col = c("yellow", "purple", "green"), cex = 2,
lwd = 3)
plot(hc_cusine.wardd2, main = "Dendrogramme des données reecettes \n Méthode de Ward")
points(c(2, 3, 8), inertie[c(2, 3, 8)], col = c("yellow", "purple", "green"), cex = 2,
lwd = 3)
plot(hc_cusine.wardd2, main = "Dendrogramme des données reecettes \n Méthode de Ward")
rect.hclust(hc_cusine.wardd2, 2, border = "yellow")
rect.hclust(hc_cusine.wardd2, 3, border = "purple")
rect.hclust(hc_cusine.wardd2, 8, border = "green")
###méthode complete
x11()
hc_cusine.complete <- hclust(dissimilarite, method = "complete")
plot(hc_cusine.complete, main = "Method : Complete")
inertie <- sort(hc_cusine.wardd2$height, decreasing = TRUE)
plot(inertie[1:20], type = "s", xlab = "Nombre de classes", ylab = "Inertie")
points(c(2, 3, 8), inertie[c(2, 3, 8)], col = c("yellow", "purple", "blue3"), cex = 2,
lwd = 3)
###méthode moyenne
x11()
hc_cusine.average <- hclust(dissimilarite, method = "average")
plot(hc_cusine.average, main = "Method : Average")
hc_cusine.median <- hclust(dissimilarite, method = "median")
plot(hc_cusine.median, main = "Method : Median")
#Execution de l'algorithm
rec_matrice <- as.matrix(recette_pays)
rec_scaled <- scale(rec_matrice)
dissimilarite <- dist(rec_scaled)
rec_km <- kmeans(rec_scaled, 2, iter.max = 1000 )
x11()
clusplot(dissimilarite, rec_km$cluster, repel=TRUE, diss= TRUE, color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Données recette: Algorithme K-means avec k = 2", xlab = "composante 1", ylab="composante 2" )
#Inertie
inertie_matrice <- matrix(0, nrow = 100, ncol = 10)
for (i in 1:10)
{
for (j in 1:100)
{
inertie_matrice[j,i] <- kmeans(rec_scaled,i)$tot.withinss
}
}
inertie <- apply(inertie_matrice,2,min)
plot(inertie, main = "Données recette: Inertie intra-classe minimale en fonction de K", type = "l", xlab = "K", ylab = "Inertie intra-classe")
apply(inertie_matrice,2,var)
#Inertie intra classe (courbe montante)
inertie_matrice <- matrix(0, nrow = 100, ncol = 10)
for (i in 1:10)
{
for (j in 1:100)
{
inertie_matrice[j,i] <- kmeans(rec_scaled,i)$betweenss/kmeans(rec_scaled,i)$totss*100
}
}
inertie <- apply(inertie_matrice,2,min)
x11()
plot(inertie, main = "Données recette: Proportion de l'inertie inter-classe sur l'inertie totale \nen fonction de K", type = "l", xlab = "K", ylab = "Inertie inter-classe")
apply(inertie_matrice,2,var)
#question 5
##AFTD
#library("ggrepel") # evite chauvauchement texte pour le plot
rec_matrice <- as.matrix(recette_pays,diag=TRUE, upper=TRUE) # -1/2 Q* D^2*Q
rec_scaled <- scale(rec_matrice) #centrage de la matrice
dissimilarite <- dist(rec_scaled, diag = TRUE)
mds <- cmdscale(dissimilarite, eig = TRUE,k=5)
#Affichage des plots
x11()
barplot(mds$eig, main= "Valeurs propres issues de l'AFTD du jeu de données de Cuisine", xlab=expression(paste("Valeurs propres ", lambda," n ")), ylab="valeurs propres")#valeurs propres issus de l'AFTD
par(mfrow=c(1,3))
abline(h = 0, v = 0, col = "blue")
library(MASS)
ggplot(as.data.frame(mds$points))+ geom_point(aes(mds$points[,1],mds$points[,2], color = 'red'))+geom_text_repel(aes(mds$points[,1],mds$points[,2],label = rownames(recette_pays))) + ggtitle("Données recette dans le premier plan factoriel")
#observation des diagrammes de shepard pour différentes valeur de k
x11()
par(mfrow=c(1,3))
#K=2
cuisine_AFTD1<- cmdscale(dissimilarite, k=2)
s1 = Shepard(as.dist(dissimilarite), cuisine_AFTD1)
plot(s1$y~s1$yf, main = "Diagramme de Shepard pour k = 2",col="black", pch =4, asp=1)
abline(0, 1, col="blue")+legend("topright", legend =paste("R²=",R), cex = 0.5)
R<-cor(s1$y,s1$yf)
#K=3
cuisine_AFTD3<- cmdscale(dissimilarite, k=3)
s3 = Shepard(as.dist(dissimilarite), cuisine_AFTD3)
plot(s3$y~s3$yf, main = "iagramme de Shepard pour k = 3)",col=rainbow(ncol(recette_pays)), pch =4, asp=1)
abline(0, 1)
cor(s3$y,s3$yf)
s5 = Shepard(as.dist(dissimilarite), cuisine_AFTD5)
plot(s5$y~s5$yf, main = "iagramme de Shepard pour k = 5)",col=rainbow(ncol(recette_pays)), pch =4, asp=1)
#K=5
cuisine_AFTD5<- cmdscale(dissimilarite, k=5)
abline(0, 1)
cor(s5$y,s5$yf)
#Donnees recettes-echant.data
setwd("D:/Bi/utc/SY09/Projet 1")
recettes_echant <- read.table("recettes-echant.data", header = TRUE, sep = ",")
str(recettes_echant)
sapply(recettes_echant[,-1],var)
covariance <- cov(recettes_echant[,-1])
summary(covariance)
correlation <- cor(recettes_echant[,-1])
#K=5
cuisine_AFTD5<- cmdscale(dissimilarite, k=5)
s5 = Shepard(as.dist(dissimilarite), cuisine_AFTD5)
plot(s5$y~s5$yf, main = "iagramme de Shepard pour k = 5)",col=rainbow(ncol(recette_pays)), pch =4, asp=1)
abline(0, 1)
cor(s5$y,s5$yf)
#Donnees recettes-echant.data
setwd("D:/Bi/utc/SY09/Projet 1")
recettes_echant <- read.table("recettes-echant.data", header = TRUE, sep = ",")
str(recettes_echant)
sapply(recettes_echant[,-1],var)
covariance <- cov(recettes_echant[,-1])
summary(covariance)
correlation <- cor(recettes_echant[,-1])
summary(covariance)
x11()
par(mfrow=c(1,2))
barplot(apply(recettes_echant[,2:51],2,sum), horiz=TRUE, space=0.5, cex.axis = 1,cex.names = 0.70, las=1, main= "Barplot des données recettes en binaire \n Somme des observations pour chaque ingrédient")
barplot(apply(recettes_echant[,2:51],2,sum), horiz=TRUE, space=0.5, cex.axis = 1,cex.names = 0.75, las=1)
#Question 7 et 8:
#Transformation
rec_trans <- matrix(0, nrow = length(unique(recettes_echant$origin)), ncol = ncol(recettes_echant[,-1]))
rownames(rec_trans) <- unique(recettes_echant[,1])
for (i in 1:length(unique(recettes_echant[,1]))) # de 1 à 26 car 26 origines
{
print(i)
for (j in 2:ncol(recettes_echant[,])) #51 ingrédients
{
count <- 0
for (k in 1:nrow(recettes_echant)) #2000 observations
{
if (recettes_echant[k,1] == unique(recettes_echant[,1])[i]) # si k=5 --> maroccan si i=5-->Indian/ le test sera faux
count <- count + recettes_echant[k,j]# si test positif alors stoke dans count (soit 0 ou 1)
}
rec_trans[i,j-1] <- count  #on stocke ensiute dans le tableau
}
}
#test<-rec_trans
test <-rec_trans
test<-read.csv("rec_trans.csv", header = TRUE, sep = ",", row.names = "X")
rec <- as.matrix(test)
rec_scaled2 <- scale(rec)#centrage pour éviter le poids des variable avec variances importantes
dissimilarite2 <- dist(rec_scaled2, method = "manhattan", diag = TRUE)
#méthode de Ward
hc2_cusine.ward <- hclust(dissimilarite2, method ="ward.D2")
x11()
plot(hc2_cusine.ward, main=" ")
title(main=paste("Dendogramme par classification ascendante hierarchique","\n",sep=""))
title(main=paste("\n","Methode de Ward",sep=""),cex.main=0.75)
rect.hclust(hc2_cusine.ward,4)
inertie2 <- sort(hc2_cusine.ward$height, decreasing = TRUE)
plot(inertie[1:20], type = "s", xlab = "Nombre de classes", ylab = "Inertie")
points(c(2, 3,4,6,7, 8), inertie[c(2, 3,4,6,7, 8)], col = c("yellow", "purple", "green", "orange", "red", "grey"), cex = 2,
lwd = 3)
###dissimilarite/ je ne suis pas sure que ce soit ça
rec3 <- t(as.matrix(recettes_echant[,-1]))
dissimilarite3 <- dist(rec3, method = "binary", diag = TRUE)
hc_rec3 <- hclust(dissimilarite3, method = "ward.D2")
plot(hc_rec3)
library("cluster")
asw <- numeric(20)
for (k in 2:20)
asw[k] <- pam(dissimilarite2, k) $ silinfo $ avg.width
k.best <- which.max(asw)
par(mfrow=c(1,2))
kmedoides4 <- pam(dissimilarite2, 4, metric = "manhattan", diss = TRUE)
clusplot(as.matrix(dissimilarite2), kmedoides4$clustering, diss= TRUE, color = TRUE , shade = TRUE, labels = 2, lines = 0,main= paste("Kmedoides avec k=4"))
kmedoides2 <- pam(dissimilarite2, 2, metric = "manhattan", diss = TRUE)
clusplot(as.matrix(dissimilarite2), kmedoides2$clustering, diss= TRUE, color = TRUE , shade = TRUE, labels = 2, lines = 0,main= paste("Kmedoides avec k=2"))
par(mfrow=c(1,2))
plot(s.pam <- silhouette(kmedoides4), main = "Silhouette de la classification des k-medoïdes pour K=4")+text(rownames(rec_scaled2))
plot(s.pam <- silhouette(kmedoides2), main = "Silhouette de la classification des k-medoïdes pour K=4")+text(rownames(rec_scaled2))
par(mfrow=c(1,2))
grid.table(Ingr_Classe<-as.data.frame(kmedoides4$medoids)) # ingrédient représentant les 4 classes
grid.table(Ingr_Classe<-as.data.frame(kmedoides2$medoids))
library(lattice)
library(corrplot)
library("FactoMineR")
library(factoextra)
library(ggplot2)
library(tibble)
library(ade4)
library(MASS)
library(ggrepel)
library(cluster)
grid.table(Ingr_Classe<-as.data.frame(kmedoides4$medoids)) # ingrédient représentant les 4 classes
grid.table(Ingr_Classe<-as.data.frame(kmedoides2$medoids))
setwd("D:/Bi/utc/sy09/Projet1 _ Anh Tu NGUYEN - Marie Valmori")
library(MASS)
library(cluster)
library(mclust)
library(factoextra)
#Donnees Synth1
X1 <- read.csv("./Synth1.csv", header=T, row.names=1)
z <- X1[,3]
X <- X1[,-3]
#plot
x11()
par(mfrow=c(1,3))
plot(X1$X.1~X1$X.2, pch=21, xlab="X.2", ylab="X.1", bg=c("red","green3")[unclass(X1$z)], main="Répartition des données synth1")
#hc
d <- dist(X, diag = TRUE, method = "manhattan")
hc <- hclust(d, method = "ward.D2")
plot(hc,  main = "Dendrogramme des données synthétiques 1\n Méthode: ward")
par(mfrow=c(1,2))
fviz_pca_ind(res.pca<-princomp(X, scale=TRUE), habillage=X1$z)
#kmeans classique
km <- kmeans(X, 2)
clusplot(X, km$cluster, color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Synth 1 : Kmeans classique avec k = 2", xlab="composante 1", ylab="composante 2")
inertie_matrice <- matrix(0, nrow = 100, ncol = 10)
for (i in 1:10)
{
for (j in 1:100)
{
inertie_matrice[j,i] <- kmeans(X,i)$tot.withinss
}
}
inertie <- apply(inertie_matrice,2,min)
plot(inertie, main = "Synth 1 :Inertie intra-classe minimale en fonction de K", type = "l", xlab = "K", ylab = "Inertie intra-classe")
apply(inertie_matrice,2,var)
# indice de rand
IndiceRand<-matrix (0, nrow = 10, ncol = 6)
colnames(IndiceRand) <-c("k1","k2","k3", "k4", "k5", "k6")
for (i in 1:6){
for (j in 1:10)
{
IndiceRand[j,i]<-adjustedRandIndex(z, kmeans(X, i)$cluster)
}
}
Rand<-apply(IndiceRand,2,max)
plot(Rand, main = "Synth 1: Indice de Rand maximal en fonction de K ", type = "l", xlab = "K", ylab = "Indice de Rand")
#kmeans adatative
source("./kmeans_adaptative.R")
X <- as.matrix(X)
ka <- kmeans_da(X, 2)
x11()
clusplot(X, ka$Cluster,color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Synth 1 : Kmeans adaptative avec k = 2", xlab="composante 1", ylab="composante 2")
comparer <- adjustedRandIndex(ka$Cluster, z)
inertie_matrice <- matrix(0, nrow = 10, ncol = 10)
for (i in 1:10)
{
for (j in 1:10)
{
ka <- kmeans_da(X, i)
inertie_matrice[j,i] <- ka$Valeur_critere
}
}
#Donnees Synth2
X2 <- read.csv("./Synth2.csv", header=T, row.names=1)
z <- X2[,3]
X <- X2[,-3]
plot(X2$X.1~X2$X.2, pch=21, xlab="X.2", ylab="X.1", bg=c("orange","blue")[unclass(X2$z)], main="Répartition des données synth2")
#hc
d <- dist(X, diag = TRUE)
hc <- hclust(d)
plot(hc, main = "Synth 2")
fviz_pca_ind(res.pca<-princomp(X, scale=TRUE), habillage=X1$z)
#kmeans classique
km <- kmeans(X, 2)
clusplot(X, km$cluster, color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Synth 2 : Kmeans classique avec k = 2")
#kmeans adatative
source("./kmeans_adaptative.R")
X <- as.matrix(X)
ka <- kmeans_da(X, 2)
clusplot(X, ka$Cluster,color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Synth 2 : Kmeans adaptative avec k = 2")
comparer <- adjustedRandIndex(z, ka$Cluster)
comparer
ka <- kmeans_da(X, 7)
clusplot(X, ka$Cluster,color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Synth 1 : Kmeans adaptative avec k = 7")
#Donnees Synth3
X3 <- read.csv("./Synth3.csv", header=T, row.names=1)
z <- X3[,3]
X <- X3[,-3]
plot(X3$X.1~X3$X.2, pch=21, xlab="X.2", ylab="X.1", bg=c("red","green3")[unclass(X3$z)], main="Répartition des données synth3")
#hc
d <- dist(X, diag = TRUE)
hc <- hclust(d)
plot(hc, main = "Synth 3")
fviz_pca_ind(res.pca<-princomp(X, scale=TRUE), habillage=X3$z)
#kmeans classique
km <- kmeans(X, 2)
clusplot(X, km$cluster, color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Synth 3 : Kmeans classique avec k = 2")
#kmeans adatative
source("./kmeans_adaptative.R")
X <- as.matrix(X)
ka <- kmeans_da(X, 2)
clusplot(X, ka$Cluster,color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Synth3 : Kmeans adaptative avec k = 2")
########Iris
data(iris)
X <- iris[,1:4]
z<-as.numeric(iris$Species) #1=setosa/ 2=versicolor/ 3=virginica
km <- kmeans(X, 1)
km <- kmeans(X, 2)
km <- kmeans(X, 3)
X<-as.matrix(X)
par(mfrow=c(2,2))
X <- as.matrix(X)
ka <- kmeans_da(X, 1)
clusplot(X, ka$Cluster, color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Kmeans adaptative \n données iris avec k = 1")
ka$Valeur_critere
ka2 <- kmeans_da(X, 2)
clusplot(X, ka2$Cluster, color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Kmeans adaptative \n données iris avec k = 2")
ka$Valeur_critere
ka <- kmeans_da(X, 3)
ka3<-ka
clusplot(X, ka3$Cluster, color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Kmeans adaptative \n données iris avec k = 3")
ka$Valeur_critere
ka4 <- kmeans_da(X, 4)
clusplot(X, ka4$Cluster, color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Kmeans adaptative \n données iris avec k = 4")
ka$Valeur_critere
ka5 <- kmeans_da(X, 5)
clusplot(X, ka5$Cluster, color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Kmeans adaptative \n données iris avec k = 5")
ka$Valeur_critere
ka <- kmeans_da(X, 2)
clusplot(X, ka$Cluster, color = TRUE , shade = TRUE, labels = 2, lines = 0, main = "Kmeans adaptative \n données iris avec k = 2")
ka$Valeur_critere
setwd("D:/Bi/utc/sy09/Projet1 _ Anh Tu NGUYEN - Marie Valmori")
source("./kmeans_adaptative.R")
library(MASS)
library(cluster)
library(mclust)
#Donnees Spam
Spam <- read.csv("spam.csv", header=T, row.names=1)
for (i in 1:54)
{
Spam[,i][which(Spam[,i]>0)] <-1
}
X <- Spam[,-58]
z <- Spam[,58]
acp = princomp(X,cor=TRUE)
summary(acp,loadings=TRUE)
biplot(acp)
screeplot(acp,type="lines")
PC = predict(acp)[,1:40]
PC <- as.matrix(PC)
km <- kmeans(PC,2)
adjustedRandIndex(z,km$cluster)
#Donnees Spam
Spam <- read.csv("spam.csv", header=T, row.names=1)
for (i in 1:54)
{
Spam[,i][which(Spam[,i]>0)] <-1
}
X <- Spam[,-58]
z <- Spam[,58]
acp = princomp(X,cor=TRUE)
summary(acp,loadings=TRUE)
biplot(acp)
screeplot(acp,type="lines")
PC = predict(acp)[,1:40]
PC <- as.matrix(PC)
km <- kmeans(PC,2)
adjustedRandIndex(z,km$cluster)
resSpam = kmeans_da(PC,2)
Spam <- read.csv("spam.csv", header=T, row.names=1)
X <- Spam[,-58]
z <- Spam[,58]
source("./knn_adaptative.R")
nTotal <- nrow(X)
Spam.training <- as.matrix(X[1 : round((nTotal *2/3)),])
Spam.test <- as.matrix(X[(round((nTotal *2/3)) + 1) : nTotal,])
Spam.target <- as.matrix(z[1 : round((nTotal *2/3))])
pred <- knn_mahalanobis(Spam.target, traindata = Spam.training, testdata = Spam.test, k = 2)
print(pred)
print(table(pred, z[-(1 : round((nTotal *2/3)))]))
res <- estimationErreur(z, X, k = c(2, 3, 4), n = 5)
print(res)
res <- estimationErreur(z, X, k = c(2, 3, 4), n = 5)
print(res)
Spam <- read.csv("spam.csv", header=T, row.names=1)
X <- Spam[,-58]
z <- Spam[,58]
res <- estimationErreur(z, X, k = c(2, 3, 4), n = 5)
print(res)
source("./knn_adaptative.R")
res <- estimationErreur(z, X, k = c(2, 3, 4), n = 5)
print(res)
source("./knn_adaptative.R")
res <- estimationErreur(z, X, k = c(2, 3, 4), n = 5)
print(res)
res <- estimationErreur(z, X, k = c(2, 3, 4), n = 5)
print(res)
X
source("./knn_adaptative.R")
nTotal <- nrow(X)
res <- estimationErreur(z, X, k = c(2, 3, 4), n = 5)
print(res)
